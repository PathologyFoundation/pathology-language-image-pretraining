# Leveraging medical Twitter to build a visual–language foundation model for pathology AI

### This Github repository contains the evaluation codes.

![PLIP](assets/banner.png "A visual–language foundation model for pathology AI")

## Links
- [Official huggingface website](https://huggingface.co/spaces/vinid/webplip)
- [PLIP model (huggingface transformers)](https://huggingface.co/vinid/plip)
- [Preprint](https://www.biorxiv.org/content/10.1101/2023.03.29.534834v1)

## Config Env File

```
PC_CACHE_FOLDER: is the folder in which cached embeddings are saved
PC_RESULTS_FOLDER: is the folder in which results are going to be saved
PC_EVALUATION_DATA_ROOT_FOLDER: this folder should point to the folder in which there are the evaluation dataset
PC_DEFAULT_BACKBONE: this should point at the backbone to use as default for PLIP
PC_CLIP_ARCH: this is the architecture for PLIP and BLIP (e.g., "ViT-B/32")
```

## Evaluation Scripts

All evaluation scripts are found in `path_eval/scripts`. They use the abstractions found in `path_eval/evaluation`

## Dataset Details

### Data Location (For internal use only)

Classification Task
`/v2/evaluation_datasets/classification`

Retrieval Task
`/v2/evaluation_datasets/image_retrieval`


### Setup Images

Data has been generated by splitting using:
`seed=1_trainratio=0.70_size=224`